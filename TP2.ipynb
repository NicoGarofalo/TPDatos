{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6bWqgOVmrjA"
   },
   "source": [
    "# Notebook oficial - TP Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKyPFVKemrAK"
   },
   "outputs": [],
   "source": [
    "# Importando librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Modelos\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La dimension del dataset es:  7613 registros, 5 columnas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carga del dataset\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "#Dimension\n",
    "print('La dimension del dataset es: ',df_train.shape[0], 'registros,', df_train.shape[1],'columnas')\n",
    "# Vista de los primeros registros\n",
    "df_train.head(5)\n",
    "# Data:\n",
    "# id - identificador unico para cada tweet\n",
    "# keyword - un keyword para el tweet (podría faltar)\n",
    "# location - ubicación desde donde fue enviado (podría no estar)\n",
    "# text - el texto del tweet\n",
    "# target - indica si se trata de un desastre real (1) o no (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de tipos\n",
    "df_train['id'] = df_train['id'].astype(int)\n",
    "df_train['keyword'] = df_train['keyword'].fillna(value = \"noKeyword\").astype('object')\n",
    "df_train['location'] = df_train['location'].astype('object')\n",
    "df_train['text'] = df_train['text'].astype('object')\n",
    "df_train['target'] = df_train['target'].astype('bool')\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['keyword'] = df_test['keyword'].fillna(value = \"noKeyword\").astype('object')\n",
    "df_test['location'] = df_test['location'].astype('object')\n",
    "df_test['text'] = df_test['text'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino duplicados\n",
    "df_train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613</td>\n",
       "      <td>5080</td>\n",
       "      <td>7613</td>\n",
       "      <td>7613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>222</td>\n",
       "      <td>3341</td>\n",
       "      <td>7503</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>noKeyword</td>\n",
       "      <td>USA</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>4342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id    keyword location  \\\n",
       "count    7613.000000       7613     5080   \n",
       "unique           NaN        222     3341   \n",
       "top              NaN  noKeyword      USA   \n",
       "freq             NaN         61      104   \n",
       "mean     5441.934848        NaN      NaN   \n",
       "std      3137.116090        NaN      NaN   \n",
       "min         1.000000        NaN      NaN   \n",
       "25%      2734.000000        NaN      NaN   \n",
       "50%      5408.000000        NaN      NaN   \n",
       "75%      8146.000000        NaN      NaN   \n",
       "max     10873.000000        NaN      NaN   \n",
       "\n",
       "                                                     text target  \n",
       "count                                                7613   7613  \n",
       "unique                                               7503      2  \n",
       "top     11-Year-Old Boy Charged With Manslaughter of T...  False  \n",
       "freq                                                   10   4342  \n",
       "mean                                                  NaN    NaN  \n",
       "std                                                   NaN    NaN  \n",
       "min                                                   NaN    NaN  \n",
       "25%                                                   NaN    NaN  \n",
       "50%                                                   NaN    NaN  \n",
       "75%                                                   NaN    NaN  \n",
       "max                                                   NaN    NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunos registros cuyo label es incorrecto, los corregimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>328</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ready to get annihilated for the BUCS game</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>443</td>\n",
       "      <td>apocalypse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short Reading\\r\\n\\r\\nApocalypse 21:1023 \\r\\n\\r...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>513</td>\n",
       "      <td>army</td>\n",
       "      <td>Studio</td>\n",
       "      <td>But if you build an army of 100 dogs and their...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2619</td>\n",
       "      <td>crashed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My iPod crashed..... \\r\\n#WeLoveYouLouis \\r\\n#...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>3640</td>\n",
       "      <td>desolation</td>\n",
       "      <td>Quilmes , Arg</td>\n",
       "      <td>This desperation dislocation\\r\\nSeparation con...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>3900</td>\n",
       "      <td>devastated</td>\n",
       "      <td>PG Chillin!</td>\n",
       "      <td>Man Currensy really be talkin that talk... I'd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>4342</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Going to a fest? Bring swimming goggles for th...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>5781</td>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campsite recommendations \\r\\nToilets /shower \\...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>6552</td>\n",
       "      <td>injury</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>My prediction for the Vikings game this Sunday...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>6554</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dante Exum's knee injury could stem Jazz's hop...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>6570</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Sport_EN Just being linked to Arsenal causes ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>6701</td>\n",
       "      <td>lava</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Imagine a room with walls that are lava lamps.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>6702</td>\n",
       "      <td>lava</td>\n",
       "      <td>probably watching survivor</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>6729</td>\n",
       "      <td>lava</td>\n",
       "      <td>Clayton, NC</td>\n",
       "      <td>Check out my Lava lamp dude ???? http://t.co/T...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>6861</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>i'm a Citizen of the World</td>\n",
       "      <td>If abortion is murder then blowjobs are cannib...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>7226</td>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>on to the next adventure</td>\n",
       "      <td>Of course the one day I have to dress professi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword                    location  \\\n",
       "229    328         annihilated                         NaN   \n",
       "301    443          apocalypse                         NaN   \n",
       "356    513                army                      Studio   \n",
       "1822  2619             crashed                         NaN   \n",
       "2536  3640          desolation               Quilmes , Arg   \n",
       "2715  3900          devastated                 PG Chillin!   \n",
       "3024  4342        dust%20storm                     chicago   \n",
       "4068  5781      forest%20fires                         NaN   \n",
       "4609  6552              injury                  Saint Paul   \n",
       "4611  6554              injury                         NaN   \n",
       "4622  6570              injury                         NaN   \n",
       "4713  6701                lava               Nashville, TN   \n",
       "4714  6702                lava  probably watching survivor   \n",
       "4732  6729                lava                 Clayton, NC   \n",
       "4820  6861       mass%20murder  i'm a Citizen of the World   \n",
       "5068  7226  natural%20disaster    on to the next adventure   \n",
       "\n",
       "                                                   text  target  \n",
       "229          Ready to get annihilated for the BUCS game    True  \n",
       "301   Short Reading\\r\\n\\r\\nApocalypse 21:1023 \\r\\n\\r...    True  \n",
       "356   But if you build an army of 100 dogs and their...    True  \n",
       "1822  My iPod crashed..... \\r\\n#WeLoveYouLouis \\r\\n#...    True  \n",
       "2536  This desperation dislocation\\r\\nSeparation con...    True  \n",
       "2715  Man Currensy really be talkin that talk... I'd...    True  \n",
       "3024  Going to a fest? Bring swimming goggles for th...    True  \n",
       "4068  Campsite recommendations \\r\\nToilets /shower \\...    True  \n",
       "4609  My prediction for the Vikings game this Sunday...    True  \n",
       "4611  Dante Exum's knee injury could stem Jazz's hop...    True  \n",
       "4622  @Sport_EN Just being linked to Arsenal causes ...    True  \n",
       "4713     Imagine a room with walls that are lava lamps.    True  \n",
       "4714  The sunset looked like an erupting volcano ......    True  \n",
       "4732  Check out my Lava lamp dude ???? http://t.co/T...    True  \n",
       "4820  If abortion is murder then blowjobs are cannib...    True  \n",
       "5068  Of course the one day I have to dress professi...    True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mislabelled_ids = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "\n",
    "df_train.loc[ df_train['id'].isin(mislabelled_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_target( tweet_id , target ):\n",
    "    \n",
    "    if tweet_id in mislabelled_ids:\n",
    "        target = False\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>328</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ready to get annihilated for the BUCS game</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>443</td>\n",
       "      <td>apocalypse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short Reading\\r\\n\\r\\nApocalypse 21:1023 \\r\\n\\r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>513</td>\n",
       "      <td>army</td>\n",
       "      <td>Studio</td>\n",
       "      <td>But if you build an army of 100 dogs and their...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2619</td>\n",
       "      <td>crashed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My iPod crashed..... \\r\\n#WeLoveYouLouis \\r\\n#...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>3640</td>\n",
       "      <td>desolation</td>\n",
       "      <td>Quilmes , Arg</td>\n",
       "      <td>This desperation dislocation\\r\\nSeparation con...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>3900</td>\n",
       "      <td>devastated</td>\n",
       "      <td>PG Chillin!</td>\n",
       "      <td>Man Currensy really be talkin that talk... I'd...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>4342</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Going to a fest? Bring swimming goggles for th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>5781</td>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campsite recommendations \\r\\nToilets /shower \\...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>6552</td>\n",
       "      <td>injury</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>My prediction for the Vikings game this Sunday...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>6554</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dante Exum's knee injury could stem Jazz's hop...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>6570</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Sport_EN Just being linked to Arsenal causes ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>6701</td>\n",
       "      <td>lava</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Imagine a room with walls that are lava lamps.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>6702</td>\n",
       "      <td>lava</td>\n",
       "      <td>probably watching survivor</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>6729</td>\n",
       "      <td>lava</td>\n",
       "      <td>Clayton, NC</td>\n",
       "      <td>Check out my Lava lamp dude ???? http://t.co/T...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>6861</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>i'm a Citizen of the World</td>\n",
       "      <td>If abortion is murder then blowjobs are cannib...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>7226</td>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>on to the next adventure</td>\n",
       "      <td>Of course the one day I have to dress professi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword                    location  \\\n",
       "229    328         annihilated                         NaN   \n",
       "301    443          apocalypse                         NaN   \n",
       "356    513                army                      Studio   \n",
       "1822  2619             crashed                         NaN   \n",
       "2536  3640          desolation               Quilmes , Arg   \n",
       "2715  3900          devastated                 PG Chillin!   \n",
       "3024  4342        dust%20storm                     chicago   \n",
       "4068  5781      forest%20fires                         NaN   \n",
       "4609  6552              injury                  Saint Paul   \n",
       "4611  6554              injury                         NaN   \n",
       "4622  6570              injury                         NaN   \n",
       "4713  6701                lava               Nashville, TN   \n",
       "4714  6702                lava  probably watching survivor   \n",
       "4732  6729                lava                 Clayton, NC   \n",
       "4820  6861       mass%20murder  i'm a Citizen of the World   \n",
       "5068  7226  natural%20disaster    on to the next adventure   \n",
       "\n",
       "                                                   text  target  \n",
       "229          Ready to get annihilated for the BUCS game   False  \n",
       "301   Short Reading\\r\\n\\r\\nApocalypse 21:1023 \\r\\n\\r...   False  \n",
       "356   But if you build an army of 100 dogs and their...   False  \n",
       "1822  My iPod crashed..... \\r\\n#WeLoveYouLouis \\r\\n#...   False  \n",
       "2536  This desperation dislocation\\r\\nSeparation con...   False  \n",
       "2715  Man Currensy really be talkin that talk... I'd...   False  \n",
       "3024  Going to a fest? Bring swimming goggles for th...   False  \n",
       "4068  Campsite recommendations \\r\\nToilets /shower \\...   False  \n",
       "4609  My prediction for the Vikings game this Sunday...   False  \n",
       "4611  Dante Exum's knee injury could stem Jazz's hop...   False  \n",
       "4622  @Sport_EN Just being linked to Arsenal causes ...   False  \n",
       "4713     Imagine a room with walls that are lava lamps.   False  \n",
       "4714  The sunset looked like an erupting volcano ......   False  \n",
       "4732  Check out my Lava lamp dude ???? http://t.co/T...   False  \n",
       "4820  If abortion is murder then blowjobs are cannib...   False  \n",
       "5068  Of course the one day I have to dress professi...   False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reemplazo los targets que estan mislabeleados\n",
    "\n",
    "df_train['target'] = df_train.apply(lambda row: relabel_target(row[\"id\"], row['target']), axis=1)\n",
    "\n",
    "df_train.loc[ df_train['id'].isin(mislabelled_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------WARNING---------------#\n",
    "\n",
    "# esta libreria solo se puede instalar si tenes JDK VERSION 8\n",
    "# si tenes cualquier otra version NO SE PUEDE INSTALAR\n",
    "\n",
    "# para instalarlo: \n",
    "# pip install pycontractions\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont = df_train.copy()\n",
    "df_test_cont = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"only had a car for not even a week and got in a fucking car accident .. Mfs can't fucking drive .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#revisamos un tweet que tiene alguna contraccion\n",
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_cont['text'] = list( cont.expand_texts(df_train_cont['text'].to_list()) )\n",
    "\n",
    "df_test_cont['text'] = list( cont.expand_texts(df_test_cont['text'].to_list()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'only had a car for not even a week and got in a fucking car accident .. Mfs cannot fucking drive .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reemplazo los urls presentes con \"URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my Lava lamp dude ???? http://t.co/To9ViqooFv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patron que siguen los urls de los tweets\n",
    "pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "\n",
    "def replace_urls(text):\n",
    " \n",
    "    replaced = re.sub(pattern, 'URL', text)\n",
    "    \n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply(lambda x: replace_urls(x))\n",
    "\n",
    "df_test_cont['text'] = df_test_cont['text'].apply(lambda x: replace_urls(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my Lava lamp dude ???? URL'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuento la cantidad de palabras que tiene cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['word_count'] = df_train_cont['text'].apply(lambda x: word_count(x))\n",
    "df_test_cont['word_count'] = df_test_cont['text'].apply(lambda x: word_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>noKeyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>noKeyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>noKeyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    keyword location                                               text  \\\n",
       "0   1  noKeyword      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4  noKeyword      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5  noKeyword      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  word_count  \n",
       "0    True          13  \n",
       "1    True           7  \n",
       "2    True          22  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my Lava lamp dude ???? URL'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nos interesaba contar los url como palabra, entonces decidimos eliminarlos despues de crear el 'word_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \n",
    "    no_url = re.sub('URL', '', text)\n",
    "    \n",
    "    return no_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply( lambda x: remove_urls(x) )\n",
    "df_test_cont['text'] = df_train_cont['text'].apply( lambda x: remove_urls(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my Lava lamp dude ???? '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de los text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_slang(tweet):\n",
    "\n",
    "    \n",
    "    # remover caracteres especiales\n",
    "\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "\n",
    "\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[=><,*;_:#@&\\']', '',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(text):\n",
    "\n",
    "    tokenizer = TweetTokenizer(reduce_len=True,strip_handles=False)\n",
    "\n",
    "    processed_text = procesar_slang(text)\n",
    "    processed_text = clean_text(processed_text)\n",
    "    processed_text = tokenizer.tokenize(processed_text) \n",
    "    processed_text = ' '.join(processed_text)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply(lambda x : pre_process_text(x))\n",
    "df_test_cont['text'] = df_test_cont['text'].apply(lambda x : pre_process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'our deeds are the reason of this earthquake may allah forgive us all'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para instalar textblob:  \n",
    "#pip install -U textblob\n",
    "\n",
    "#para instalar los datos para usar textblob: \n",
    "#python -m textblob.download_corpora\n",
    "\n",
    "from textblob import Word\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizar_texto(text):\n",
    "    \n",
    "    lem = []\n",
    "    \n",
    "    for i in text.split():\n",
    "        word1= Word(i).lemmatize(\"n\")\n",
    "        word2= Word(word1).lemmatize(\"v\")\n",
    "        word3= Word(word2).lemmatize(\"a\")\n",
    "        lem.append(Word(word3).lemmatize())\n",
    "    \n",
    "    lem_text = \" \".join(lem)\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'only had a car for not even a week and got in a fucking car accident .. mfs cannot fucking drive .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply(lambda x: lematizar_texto(x))\n",
    "df_test_cont['text'] = df_test_cont['text'].apply(lambda x: lematizar_texto(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'only have a car for not even a week and get in a fuck car accident .. mf cannot fuck drive .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Final: Red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abrimos y preparamos el pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abro el pre-trained embedding y me creo un diccionario que contenga\n",
    "# todos sus elementos\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('data/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparamos el texto del train para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entreno el tokenizer con \n",
    "\n",
    "word_tok = Tokenizer(filters='=><*_,;:#@&\\'')\n",
    "\n",
    "word_tok.fit_on_texts(df_train_cont['text'])\n",
    "\n",
    "\n",
    "#defino el vocab length (cant. de unique words +1 )\n",
    "vocab_length = len(word_tok.word_index) + 1\n",
    "\n",
    "#encodeo los tweets\n",
    "embedded_train_text = word_tok.texts_to_sequences(df_train_cont['text'])\n",
    "\n",
    "#cuantas palabras tiene el tweet mas largo\n",
    "longest_text = max( df_train_cont['word_count'] )\n",
    "\n",
    "#agrego padding para que la longitud de todos los tweets sea de 'longest_text'\n",
    "padded_train_text = pad_sequences(embedded_train_text, longest_text, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo la matriz de weights, que solo contiene los embeddings\n",
    "# de las palabras que hay en el X_train\n",
    "\n",
    "weights_matrix = np.zeros((vocab_length, 100))\n",
    "\n",
    "for word, index in word_tok.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        weights_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casteo el target a int\n",
    "targets = df_train_cont['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[weights_matrix], input_length=longest_text, trainable=True)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(6, dropout= 0.2)))\n",
    "\n",
    "model.add(Dense(units=3, activation='relu'))\n",
    "model.add(Dense(units=3, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 34, 100)           1352900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 12)                5136      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 39        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 1,358,091\n",
      "Trainable params: 1,358,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_train_text, targets, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le doy formato al texto del test para poder predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_test = word_tok.texts_to_sequences(df_test_cont['text'])\n",
    "padded_test = pad_sequences(embedding_test, longest_text, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#las predictions es un array de listas, donde cada una tiene la prediccion,\n",
    "# aca lo convertimos en una sola lista que contiene todas las predicciones\n",
    "predictions.tolist()\n",
    "form_predictions =[]\n",
    "\n",
    "for pred in predictions:\n",
    "    for target in pred:\n",
    "        form_predictions.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le doy el formato que pide kaggle y guardo el csv\n",
    "pred = pd.DataFrame()\n",
    "pred['id'] = df_test_cont['id']\n",
    "pred['target'] = form_predictions\n",
    "\n",
    "pred.to_csv('results/resultKeras.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "TP1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
