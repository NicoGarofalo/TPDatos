{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6bWqgOVmrjA"
   },
   "source": [
    "# Notebook oficial - TP Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKyPFVKemrAK"
   },
   "outputs": [],
   "source": [
    "# Importando librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Modelos\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Carga del dataset\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "#Dimension\n",
    "print('La dimension del dataset es: ',df_train.shape[0], 'registros,', df_train.shape[1],'columnas')\n",
    "# Vista de los primeros registros\n",
    "df_train.head(5)\n",
    "# Data:\n",
    "# id - identificador unico para cada tweet\n",
    "# keyword - un keyword para el tweet (podría faltar)\n",
    "# location - ubicación desde donde fue enviado (podría no estar)\n",
    "# text - el texto del tweet\n",
    "# target - indica si se trata de un desastre real (1) o no (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de tipos\n",
    "df_train['id'] = df_train['id'].astype(int)\n",
    "df_train['keyword'] = df_train['keyword'].fillna(value = \"noKeyword\").astype('object')\n",
    "df_train['location'] = df_train['location'].astype('object')\n",
    "df_train['text'] = df_train['text'].astype('object')\n",
    "df_train['target'] = df_train['target'].astype('bool')\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['keyword'] = df_test['keyword'].fillna(value = \"noKeyword\").astype('object')\n",
    "df_test['location'] = df_test['location'].astype('object')\n",
    "df_test['text'] = df_test['text'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino duplicados\n",
    "df_train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunos registros cuyo label es incorrecto, entonces hay que corregirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabelled_ids = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "\n",
    "df_train.loc[ df_train['id'].isin(mislabelled_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_target( tweet_id , target ):\n",
    "    \n",
    "    if tweet_id in mislabelled_ids:\n",
    "        target = False\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reemplazo los targets que estan mislabeleados (lo saque de algunos notebooks de kaggle)\n",
    "\n",
    "df_train['target'] = df_train.apply(lambda row: relabel_target(row[\"id\"], row['target']), axis=1)\n",
    "\n",
    "df_train.loc[ df_train['id'].isin(mislabelled_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------WARNING---------------#\n",
    "\n",
    "# esta libreria solo se puede instalar si tenes JDK VERSION 8\n",
    "# si tenes cualquier otra version NO SE PUEDE INSTALAR\n",
    "\n",
    "# para instalarlo: \n",
    "# pip install pycontractions\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esto me tarda como 6 minutos\n",
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont = df_train.copy()\n",
    "df_test_cont = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revisamos un tweet que tiene alguna contraccion\n",
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expandir todas las contractions tarda como 4 mins\n",
    "\n",
    "df_train_cont['text'] = list( cont.expand_texts(df_train_cont['text'].to_list()) )\n",
    "\n",
    "df_test_cont['text'] = list( cont.expand_texts(df_test_cont['text'].to_list()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reemplazo lo urls presentes con \"URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#este es el patron que siguen todos los urls de los tweets (creo, porahi se me escaparon algunos)\n",
    "pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "\n",
    "def replace_urls(text):\n",
    " \n",
    "    replaced = re.sub(pattern, 'URL', text)\n",
    "    \n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply(lambda x: replace_urls(x))\n",
    "\n",
    "df_test_cont['text'] = df_test_cont['text'].apply(lambda x: replace_urls(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuento la cantidad de palabras que tiene cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['word_count'] = df_train_cont['text'].apply(lambda x: word_count(x))\n",
    "df_test_cont['word_count'] = df_test_cont['text'].apply(lambda x: word_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nos interesaba contar los url como palabra, entonces decidimos eliminarlos despues de crear el 'word_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \n",
    "    no_url = re.sub('URL', '', text)\n",
    "    \n",
    "    return no_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply( lambda x: remove_urls(x) )\n",
    "df_test_cont['text'] = df_train_cont['text'].apply( lambda x: remove_urls(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[4732,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de los text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_slang(tweet):\n",
    "\n",
    "    \n",
    "    # remover caracteres especiales\n",
    "\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "\n",
    "\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[=><,*;_:#@&\\']', '',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(text):\n",
    "\n",
    "    tokenizer = TweetTokenizer(reduce_len=True,strip_handles=False)\n",
    "\n",
    "    processed_text = procesar_slang(text)\n",
    "    processed_text = clean_text(processed_text)\n",
    "    processed_text = tokenizer.tokenize(processed_text) \n",
    "    processed_text = ' '.join(processed_text)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply(lambda x : pre_process_text(x))\n",
    "df_test_cont['text'] = df_test_cont['text'].apply(lambda x : pre_process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para instalar textblob:  \n",
    "#pip install -U textblob\n",
    "\n",
    "#para instalar los datos para usar textblob: \n",
    "#python -m textblob.download_corpora\n",
    "\n",
    "from textblob import Word\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizar_texto(text):\n",
    "    \n",
    "    lem = []\n",
    "    \n",
    "    for i in text.split():\n",
    "        word1= Word(i).lemmatize(\"n\")\n",
    "        word2= Word(word1).lemmatize(\"v\")\n",
    "        word3= Word(word2).lemmatize(\"a\")\n",
    "        lem.append(Word(word3).lemmatize())\n",
    "    \n",
    "    lem_text = \" \".join(lem)\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont['text'] = df_train_cont['text'].apply(lambda x: lematizar_texto(x))\n",
    "df_test_cont['text'] = df_test_cont['text'].apply(lambda x: lematizar_texto(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cont.iloc[99,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Final: Red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abrimos y preparamos el pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abro el pre-trained embedding y me creo un diccionario que contenga\n",
    "# todos sus elementos\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('data/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparamos el texto del train para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entreno el tokenizer con \n",
    "\n",
    "word_tok = Tokenizer(filters='=><*_,;:#@&\\'')\n",
    "\n",
    "word_tok.fit_on_texts(df_train_cont['text'])\n",
    "\n",
    "\n",
    "#defino el vocab length (cant. de unique words +1 )\n",
    "vocab_length = len(word_tok.word_index) + 1\n",
    "\n",
    "#encodeo los tweets\n",
    "embedded_train_text = word_tok.texts_to_sequences(df_train_cont['text'])\n",
    "\n",
    "#cuantas palabras tiene el tweet mas largo\n",
    "longest_text = max( df_train_cont['word_count'] )\n",
    "\n",
    "#agrego padding para que la longitud de todos los tweets sea de 'longest_text'\n",
    "padded_train_text = pad_sequences(embedded_train_text, longest_text, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo la matriz de weights, que solo contiene los embeddings\n",
    "# de las palabras que hay en el X_train\n",
    "\n",
    "weights_matrix = np.zeros((vocab_length, 100))\n",
    "\n",
    "for word, index in word_tok.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        weights_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casteo el target a int\n",
    "targets = df_train_cont['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[weights_matrix], input_length=longest_text, trainable=True)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(6, dropout= 0.2)))\n",
    "\n",
    "model.add(Dense(units=3, activation='relu'))\n",
    "model.add(Dense(units=3, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_train_text, targets, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le doy formato al texto del test para poder predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_test = word_tok.texts_to_sequences(df_test_cont['text'])\n",
    "padded_test = pad_sequences(embedding_test, longest_text, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#las predictions es un array de listas, donde cada una tiene la prediccion,\n",
    "# aca lo convertimos en una sola lista que contiene todas las predicciones\n",
    "predictions.tolist()\n",
    "form_predictions =[]\n",
    "\n",
    "for pred in predictions:\n",
    "    for target in pred:\n",
    "        form_predictions.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le doy el formato que pide kaggle y guardo el csv\n",
    "pred = pd.DataFrame()\n",
    "pred['id'] = df_test_cont['id']\n",
    "pred['target'] = form_predictions\n",
    "\n",
    "pred.to_csv('results/resultKeras.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "TP1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
